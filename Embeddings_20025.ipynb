{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NLP-lab :  Word embeddings\n",
    "\n",
    "In this series of exercises, we will explore three word embeddings:\n",
    "\n",
    "* [Collobert & Weston](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) https://ronan.collobert.com/senna/\n",
    "* [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    "* [BERT](https://huggingface.co/bert-base-uncased) \n",
    "\n",
    "\n",
    "In the code already provided, add your code to the place indicated by `YOUR CODE HERE`.\n",
    "\n",
    "**Important** : do NOT commit the data and embedding files in your repository git : it is a waste of resources and it takes more time to clone.\n",
    "> Use https://docs.github.com/en/get-started/getting-started-with-git/ignoring-files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel==6.29.5 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (6.29.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.6.1)\n",
      "Requirement already satisfied: matplotlib==3.10.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Collecting gensim==4.3.3 (from -r requirements.txt (line 7))\n",
      "  Using cached gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (8.32.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 1)) (5.14.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.6.1->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0->-r requirements.txt (line 5)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0->-r requirements.txt (line 5)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0->-r requirements.txt (line 5)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0->-r requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.12/site-packages (from seaborn==0.13.2->-r requirements.txt (line 6)) (2.2.3)\n",
      "Collecting numpy (from -r requirements.txt (line 2))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy (from -r requirements.txt (line 3))\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim==4.3.3->-r requirements.txt (line 7))\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.29.5->-r requirements.txt (line 1)) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn==0.13.2->-r requirements.txt (line 6)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn==0.13.2->-r requirements.txt (line 6)) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.10.0->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim==4.3.3->-r requirements.txt (line 7)) (1.17.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.5->-r requirements.txt (line 1)) (0.2.3)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.1\n",
      "    Uninstalling scipy-1.15.1:\n",
      "      Successfully uninstalled scipy-1.15.1\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# display matplotlib graphics in notebook\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# disable warnings for libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# configure logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  Embeddings exploration with Collobert's embeddings\n",
    "\n",
    "Upload the files containing the embeddings to `data`:\n",
    "* Collobert (size 50): [collobert_embeddings.txt.zip](https://storage.teklia.com/shared/deepnlp-labs/collobert_embeddings.txt.zip) which contains the embedding vectors and [collobert_words.lst](https://storage.teklia.com/shared/deepnlp-labs/collobert_words.lst) which contains the associated words;\n",
    "\n",
    "You need to unzip the files to load them.\n",
    "\n",
    "Feel free to open the files to see what they contain (it's sometimes surprising).\n",
    "\n",
    "#### Question: \n",
    ">* Add the files to your .gitignore\n",
    ">* Give the size in Mb of the embeddings files before unzipping.\n",
    ">* By exploring the content of the embedding files, give the number of words for which these files provide embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of closest words\n",
    "\n",
    "The aim of this exercise is to list the closest words to a given word for the Collobert embedding. First, we'll load the vectors of the Collobert embedding into a numpy array and the associated words into a python list. Then we'll use the [scipy KDTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) data structure to quickly search for the vectors closest to a series of words.\n",
    "\n",
    "\n",
    "#### Question: \n",
    ">* load embedding vectors from the file `data/collobert_embeddings.txt` using the numpy function [genfromtxt](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html)\n",
    ">* load the words associated with the vectors from the `data/collobert_words.lst` file into a python list (using `open()` and `readlines()`)\n",
    ">* check that the sizes are correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# YOUR CODE HERE\n",
    "collobert_emb = np.genfromtxt(\"collobert_embeddings.txt\")\n",
    "\n",
    "with open(\"collobert_words.lst\", \"r\", encoding=\"utf-8\") as f:\n",
    "    collobert_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "assert collobert_emb.shape[0] == len(collobert_words), \"Mismatch between words and embeddings!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KD trees are a very efficient data structure for storing large sets of points in a multi-dimensional space and performing very efficient nearest-neighbour searches. \n",
    "\n",
    "#### Question \n",
    "> * Initialise the [KDTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) structure with Collobert's embedding vectors.\n",
    "> * Using the [tree.query](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query.html#scipy.spatial.KDTree.query) function, display the 5 nearest words for the following words: ‘mother’, ‘computer’, ‘dentist’, ‘war’, ‘president’, ‘secretary’, ‘nurse’.  *Hint: you can use the function `collobert_words.index(w)` to obtain the index of a word in the list of words*.\n",
    "> * Create a `words_plus_neighbors` list containing the words and all their neighbours (for the next question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of words_plus_neighbors:\n",
      "['mother', 'daughter', 'wife', 'father', 'husband', 'son']\n",
      "['computer', 'laptop', 'multimedia', 'desktop', 'software', 'wiki']\n",
      "['dentist', 'pharmacist', 'midwife', 'physician', 'housekeeper', 'veterinarian']\n",
      "['war', 'revolution', 'death', 'court', 'independence', 'history']\n",
      "['president', 'governor', 'chairman', 'mayor', 'secretary', 'senator']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "# YOUR CODE HERE\n",
    "tree = spatial.KDTree(collobert_emb)\n",
    "\n",
    "target_words = [\"mother\", \"computer\", \"dentist\", \"war\", \"president\", \"secretary\", \"nurse\"]\n",
    "\n",
    "def closest_words(word, k=5):\n",
    "    \"\"\"Retourne les k mots les plus proches d'un mot donné.\"\"\"\n",
    "    if word not in collobert_words:\n",
    "        return f\"Word '{word}' not found in vocabulary.\"\n",
    "\n",
    "    word_index = collobert_words.index(word)\n",
    "    word_vector = collobert_emb[word_index]\n",
    "\n",
    "    # Trouver les indices des k+1 plus proches voisins (inclut le mot lui-même)\n",
    "    distances, indices = tree.query(word_vector, k=k+1)\n",
    "\n",
    "    # Retourne une liste contenant le mot + ses voisins (excluant lui-même)\n",
    "    return [word] + [collobert_words[i] for i in indices[1:]]\n",
    "\n",
    "words_plus_neighbors = [closest_words(word, 5) for word in target_words]\n",
    "\n",
    "print(\"\\nExample of words_plus_neighbors:\")\n",
    "for i in range(5): \n",
    "    print(words_plus_neighbors[i])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation with T-SNE\n",
    "\n",
    "Embeddings are vectors with several hundred dimensions. It is therefore not possible to display them in their original space. However, it is possible to apply dimension reduction algorithms to display them in 2 or 3 dimensions. One of the dimension reduction algorithms allowing 2D visualisation is [tSNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). \n",
    "\n",
    "#### Question\n",
    "> * Create a `word_vectors` object of type `np.array` from a list containing all the embeddings of the words in the `words_plus_neighbors` list.\n",
    "> * Create a tSNE object from the `from sklearn.manifold import TSNE` library with the parameters `random_state=0`, `n_iter=2000` and `perplexity=15.0` for a 2-dimensional view.\n",
    "> * Calculate *T* the tSNE transformation of the `word_vectors` by applying function `.fit_transform(word_vectors)` to the tSNE object. This function estimates the parameters of the tSNE transformation and returns the reduced-dimension representation of the vectors used for estimation.\n",
    "> * Use the `scatterplot` function from [seaborn](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) to represent points in 2 dimensions and add word labels using the `plt.annotate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "# display matplotlib graphics in notebook\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# retrieve the word representation\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# create the tSNE transform\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# fit and transform the word vectors, store in T\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "sns.set(rc={'figure.figsize':(14, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "sns.scatterplot(x=T[:, 0], y=T[:, 1])\n",
    "\n",
    "for label, x, y in zip(words_plus_neighbors, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic arithmetic with Word2Vec\n",
    "\n",
    "One of the most original properties of Word2Vec embeddings is that the semantic relationships between vectors can be modelled by arithmetic operations. Given vectors representing the words `king`, `man` and `woman`, it is possible to compute the vector `v` as :  \n",
    "\n",
    "`v = vector(king)-vector(man)+vector(woman)`\n",
    "\n",
    "This operation corresponds to the following semantic relationship: *The king is to the man what the queen is to the woman*, which translates into the following arithmetic: *the concept of king, minus the concept of man plus the concept of woman gives the concept of queen*.\n",
    "\n",
    "In fact, if we look in the embedding for the word whose closest vector is `v`, we find `reine`.\n",
    "\n",
    "\n",
    "We will use a Word2Vec model pre-trained on the French Wac corpus.  This model has been trained on a corpus of 1 billion French words. \n",
    "\n",
    "This embedding is available in 2 formats:\n",
    "- a text format for easy exploration of the model :\n",
    "    - frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.txt](https://storage.teklia.com/shared/deepnlp-labs/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.txt)\n",
    "- a binary format that can be loaded using the Gensim library: \n",
    "    - [enWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin](https://storage.teklia.com/shared/deepnlp-labs/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin)\n",
    "\n",
    "Download the text file onto your machine to analyse it.\n",
    "\n",
    "#### Question: \n",
    ">* Add the file to your .gitignore\n",
    ">* Give the size in Mb of the embedding files\n",
    ">* By exploring the contents of the embedding file in text format, give the number of words for which this model provides embeddings and the size of the embedding for each word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word similarity\n",
    "\n",
    "We are now going to use the [Gensim] library (https://radimrehurek.com/gensim/) to load the Word2Vec model and use it. \n",
    "\n",
    "#### Question: \n",
    ">* Modify the following code to load the Word2Vec template file in binary format using [load_word2vec](https://radimrehurek.com/gensim/models/keyedvectors.html#how-to-obtain-word-vectors)\n",
    ">* Choose a couple of words and find the closest words according to the model using [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)\n",
    ">* To guess the meaning of the words ‘yokohama’, ‘kanto’ and ‘shamisen’, look for their nearest neighbours. Explain the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## YOUR CODE HERE\n",
    "embedding_file =\"data/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(embedding_file, binary=True, unicode_errors=\"ignore\")\n",
    "## YOUR CODE HERE\n",
    "model.most_similar(\"chevalier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic arithmetic\n",
    "\n",
    "One of the most original properties of Word2Vec embeddings is that the semantic relationships between vectors can be modelled by arithmetic operations. Given vectors representing the words `king`, `man` and `woman`, it is possible to compute the vector `v` as :  \n",
    "\n",
    "`v = vector(king)-vector(man)+vector(woman)`\n",
    "\n",
    "This operation corresponds to the following semantic relationship: *The king is to the man what the queen is to the woman*, which translates into the following arithmetic: *the concept of king, minus the concept of man plus the concept of woman gives the concept of queen*.\n",
    "\n",
    "In fact, if we look in the embedding for the word whose closest vector is `v`, we find `reine`.\n",
    "\n",
    "#### Question: \n",
    ">* using the function [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) specifying the arguments `positive` for the vectors to be added and `negative` for the vectors to be subtracted, check the relationship *the concept of king, minus the concept of man plus the concept of woman gives the concept of queen*.\n",
    ">* Using the same method, find XXX in the following semantic relations\n",
    ">   * Paris is to France what XXX is to Japan.\n",
    ">   * Chevalier is to France what XXX is to Japan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings with BERT \n",
    "\n",
    "BERT was one of the first freely available Transformer language models, trained on large corpora. Many other models are available on HuggingFace.\n",
    "\n",
    "As BERT is a contextual model, it is necessary to have it predict whole sentences in order to study the word embeddings it produces. In this section, we will compare the embeddings obtained for polysemous words according to the sentence in which they are used.\n",
    "\n",
    "In English, *plant* has two meanings: plant and vegetable. With a non-contextual embedding, such as Glove or Colobert, these two meanings of the word plus are associated with an identical embedding. With BERT, we'll see that the same word can have several embeddings depending on the context.\n",
    "\n",
    "First, load the BERT model and tokenizer from HuggingFace : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model \n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # to access the hidden states\n",
    "                                  )\n",
    "# set the model to \"evaluation\" mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Language models are trained with a specific breakdown of sentences into tokens. These tokens can be words or parts of words. It is necessary to use the tokenizer corresponding to each model.\n",
    "\n",
    "tokenizer.vocab.keys() gives the list of all the tokens known for the language model. \n",
    "\n",
    "#### Question\n",
    ">* How many different tokens are known to the BERT tokenizer?\n",
    ">* Display a hundred tokens at random. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# number of token in tokenizer\n",
    "# YOU CODE HERE\n",
    "# sample of 100 tokens\n",
    "# YOU CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer splits sentences and transforms the elements (words or sub-words) into clues. \n",
    "\n",
    "BERT can process several sentences, but you need to tell it how the sentences (segments) have been split, with an index: 0 for the first sentence, 1 for the second. \n",
    "\n",
    "Two specific tokens must also be added: \n",
    "* CLS], a specific token used for sentence classification\n",
    "* SEP], the end of sentence token.\n",
    "\n",
    "#### Question\n",
    ">* Apply the bert_tokenize function to the 3 phases and keep the 3 vectors (index, token, segment).\n",
    ">* Display this information for each of the sentences and check that the word *plant* has the same token index in the two sentences in which it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt1 = \"The plant has reached its maximal level of production.\"\n",
    "snt2 = \"The cars are assembled inside the factory.\"\n",
    "snt3 = \"A plant needs sunlight and water to grow well.\"\n",
    "\n",
    "\n",
    "def bert_tokenize(snt):\n",
    "    \"\"\" Apply the BERT tokenizer to a list of words representing a sentence\n",
    "        and return 3 lists: \n",
    "        - list of token indx\n",
    "        - list of token for debugging, not used by the BERT model\n",
    "        - list of sentence index\n",
    "        \"\"\"\n",
    "    # Add the special tokens.\n",
    "    tagged_snt = \"[CLS] \" + snt + \" [SEP]\" \n",
    "    # Tokenize\n",
    "    tokenized_snt = tokenizer.tokenize(tagged_snt)\n",
    "    # convert tokens to indices\n",
    "    indexed_snt = tokenizer.convert_tokens_to_ids(tokenized_snt)\n",
    "    # mark the words in sentence.\n",
    "    segments_ids = [1] * len(tokenized_snt)\n",
    "\n",
    "    return (indexed_snt, tokenized_snt, segments_ids)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To calculate embeddings, we need to make a prediction using the BERT model on a complete sentence. The *predict_hidden* function converts the token and segment index lists into a pytorch tensor and applies the model. \n",
    "\n",
    "The model used is a 12-layer model. We will use the last hidden layer of the model as an embedding to represent the words. Other solutions are possible, such as concatenation or averaging of several layers.\n",
    "\n",
    "\n",
    "#### Question\n",
    ">* Apply the model to each of the 3 sentences and store the resulting embeddings (tensors).\n",
    ">* Display the dimension of the resulting tensors. What is the dimension of the embedding vector for each word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_hidden(indexed_snt, segments_ids):\n",
    "    \"\"\"Apply the BERT model to the input token indices and segment indices\n",
    "        and return the last hidden layer\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_snt])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        one_hidden_layer = hidden_states[12][0]\n",
    "        \n",
    "    return one_hidden_layer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer returned by the *predict_hidden* function is a tensor containing a context vector representing each token in the input sentence. We can use this vector to represent the meaning of this word as a function of its context. We're going to compare the representation of the polysemous word *plant* as a function of its context.\n",
    "\n",
    "#### Question\n",
    ">* Using the [cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html), calculate the following distances:\n",
    "> * distance between *plant* in sentence 1 (plant-factory) and *plant* in sentence 3 (plant-vegetal)\n",
    "> * distance between *plant* in sentence 1 (plant-factory) and *factory* in sentence 2 (plant-vegetal) \n",
    "> * distance between *plant* in sentence 1 (plant-factory) and *production* in sentence 2 \n",
    "> distance between *plant* in sentence 3 (plant-vegetal) and *production* in sentence 2 \n",
    "> How can we interpret these distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
