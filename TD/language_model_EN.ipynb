{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30626a2",
   "metadata": {},
   "source": [
    "# NLP Lab: Language Models\n",
    "\n",
    "In this lab, we will build the main components of the GPT-2 model and train a small model on poems by Victor Hugo.\n",
    "\n",
    "The questions are included in this notebook. To run the training, you will need to modify the `gpt_single_head.py` file, which is also available in the Git repository.\n",
    "\n",
    "## Data\n",
    "\n",
    "The training data consists of a collection of poems by Victor Hugo, sourced from [gutenberg.org](https://www.gutenberg.org/). The dataset is available in the `data` directory.\n",
    "\n",
    "To reduce model complexity, we will model the text at the character level. Typically, language models process sequences of subwords using [tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary) such as BPE, SentencePiece, or WordPiece.\n",
    "\n",
    "#### Questions:\n",
    "- Using [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter), display the number of unique characters in the text and the frequency of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c312e9-3d94-455c-8362-9ff2974f295d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.18.0 mpmath-1.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c5dfab5-f898-429e-a058-6d9b8027751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipykernel\n",
    "import numpy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d4ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the file: 285222\n",
      "Number of character in counter: 285222\n",
      "101 different characters\n",
      "Counter({' ': 49127, 'e': 30253, 's': 17987, 'u': 14254, 'r': 14223, 't': 14071, 'a': 14048, 'n': 13725, 'i': 12828, 'o': 12653, 'l': 11638, '\\n': 8102, 'm': 6495, 'd': 6375, ',': 6077, 'c': 5074, 'p': 4206, \"'\": 3820, 'v': 3492, 'é': 2943, 'b': 2783, 'f': 2772, 'h': 2221, 'q': 1956, 'g': 1790, '.': 1420, 'x': 1154, 'L': 1147, '!': 1121, 'E': 1074, ';': 1043, '-': 1020, 'j': 890, 'D': 764, 'è': 725, 'à': 706, 'y': 660, 'I': 627, 'ê': 605, 'C': 593, 'S': 545, 'A': 530, 'Q': 503, 'z': 482, 'J': 471, 'O': 450, 'T': 441, 'P': 435, '?': 388, 'V': 383, 'â': 381, 'N': 362, 'M': 344, 'ù': 298, ':': 294, 'R': 240, 'î': 214, 'U': 208, 'ô': 159, 'X': 150, '1': 146, 'H': 116, 'F': 114, '5': 111, '8': 93, 'B': 78, '«': 74, 'É': 70, '»': 69, 'G': 67, '4': 64, 'û': 62, '3': 47, 'ç': 34, 'À': 33, 'ë': 32, 'ï': 31, '2': 30, '·': 26, 'Ê': 24, '6': 23, '7': 23, 'Ô': 19, '9': 19, 'È': 11, 'k': 10, '0': 10, '_': 8, 'Z': 7, 'Æ': 4, '[': 4, ']': 4, 'w': 3, 'K': 3, 'Y': 3, 'Ë': 2, '(': 2, ')': 2, 'Â': 2, 'Î': 1, 'W': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "with open('data/hugo_contemplations.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Number of characters in the file: {len(text)}')\n",
    "##  YOUR CODE HERE\n",
    "counter = collections.Counter(text)  # Compte la fréquence de chaque caractère\n",
    "chars = set(text)  # Ensemble des caractères uniques\n",
    "###\n",
    "\n",
    "#Définition de vocab_size\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print (f'Number of character in counter: {sum(counter.values())}')\n",
    "print (f'{len(chars)} different characters')\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b661f",
   "metadata": {},
   "source": [
    "### Encoding / Decoding  \n",
    "\n",
    "To transform the text into a vector for the neural network, each character must be encoded as an integer.  \n",
    "\n",
    "The following functions perform the encoding and decoding of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9c974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: transform a string into a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: transform a list of integers into a string\n",
    "\n",
    "\n",
    "# test that your encoder/decoder is coherent\n",
    "testString = \"\\nDemain, dès l'aube\"\n",
    "assert decode(encode (testString)) ==  testString "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a633d",
   "metadata": {},
   "source": [
    "### Train/Validation Split  \n",
    "\n",
    "Since the goal is to predict poems, the lines should not be shuffled randomly. Instead, we must preserve the order of the lines in the text and take only the first 90% for training, while using the remaining 10% to monitor learning.  \n",
    "\n",
    "#### Questions:  \n",
    "- Split the data into `train_data` (90%) and `val_data` (10%) using slicing on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5b7420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données : 285222\n",
      "Taille des données d'entraînement : 256699\n",
      "Taille des données de validation : 28523\n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'indice de séparation (90% des données pour l'entraînement)\n",
    "split_idx = int(0.9 * len(encode(text)))\n",
    "\n",
    "# Découpage des données en train et validation\n",
    "train_data = encode(text)[:split_idx]  # 90% des données\n",
    "val_data = encode(text)[split_idx:]  # 10% des données\n",
    "\n",
    "###\n",
    "\n",
    "# Vérification des tailles\n",
    "print(f'Taille des données : {len(text)}')\n",
    "print(f'Taille des données d\\'entraînement : {len(train_data)}')\n",
    "print(f'Taille des données de validation : {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa498280",
   "metadata": {},
   "source": [
    "### Context  \n",
    "\n",
    "The language model has a parameter that defines the maximum context size to consider when predicting the next character. This context is called `block_size`. The training data consists of sequences of consecutive characters, randomly sampled from the training set, with a length of `block_size`.  \n",
    "\n",
    "If the starting character of the sequence is `i`, then the context sequence is:  \n",
    "```python\n",
    "x = data[i:i+block_size]\n",
    "```\n",
    "And the target value to predict at each position in the context is the next character:  \n",
    "```python\n",
    "y = data[i+1:i+block_size+1]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a262bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([234324])\n",
      "context is >e< target is >l<\n",
      "context is >el< target is >l<\n",
      "context is >ell< target is >e<\n",
      "context is >elle< target is > <\n",
      "context is >elle < target is >f<\n",
      "context is >elle f< target is >a<\n",
      "context is >elle fa< target is >i<\n",
      "context is >elle fai< target is >t<\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "i  = torch.randint(len(encode(text)) - block_size, (1,))\n",
    "print (i)\n",
    "x = train_data[i:i+block_size]\n",
    "y = train_data[i+1:i+1+block_size]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    \n",
    "    # Si x et y sont des tenseurs PyTorch\n",
    "    if isinstance(context, torch.Tensor):\n",
    "        context_list = context.tolist()\n",
    "    else:\n",
    "        context_list = context\n",
    "        \n",
    "    if isinstance(target, torch.Tensor):\n",
    "        target_list = target.item() if target.numel() == 1 else target.tolist()\n",
    "    else:\n",
    "        target_list = target\n",
    "        \n",
    "    print(f'context is >{decode(context_list)}< target is >{decode([target_list])}<')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de81464",
   "metadata": {},
   "source": [
    "### Defining Batches  \n",
    "\n",
    "The training batches consist of multiple character sequences randomly sampled from `train_data`. To randomly select a sequence for the batch, we need to randomly pick a starting point in `train_data` and extract the following `block_size` characters. When selecting the starting point, ensure that there are enough characters remaining after it to form a full sequence of `block_size` characters.  \n",
    "\n",
    "#### Questions:  \n",
    "- Create the batches `x` by selecting `batch_size` sequences of length `block_size` starting from a randomly chosen index `i`. Stack the examples using `torch.stack`.  \n",
    "- Create the batches `y` by adding the next character following each sequence in `x`. Stack the examples using `torch.stack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9be91965",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "torch.manual_seed(2023)\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ## YOUR CODE HERE\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    ### \n",
    "    # send data and target to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507313b",
   "metadata": {},
   "source": [
    "### First Model: A Bigram Model  \n",
    "\n",
    "The first model we will implement is a bigram model. It predicts the next character based only on the current character. This model can be stored in a simple matrix: for each character (row), we store the probability distribution over all possible next characters (columns). This can be implemented using a simple [`Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer in PyTorch.  \n",
    "\n",
    "#### Questions:  \n",
    "- In the constructor, define an Embedding layer of size `vocab_size × vocab_size`.  \n",
    "- In the `forward` method, apply the embedding layer to the batch of indices (`x`).  \n",
    "- In the `forward` method, define the loss as `cross_entropy` between the predictions and the target (`y`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b7a7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# use a gpu if we have one\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # we use a simple vocab_size times vocab_size tensor to store the probabilities \n",
    "        # of each token given a single token as context in nn.Embedding\n",
    "        # YOUR CODE HERE\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        ## \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # idx and targets are both (Batch,Time) tensor of integers\n",
    "        # YOUR CODE HERE\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        ## \n",
    "   \n",
    "        # don't compute loss if we don't have targets\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # change the shape of the logits and target to match what is needed for CrossEntropyLoss\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "            Batch, Time, Channels = logits.shape\n",
    "            logits = logits.view(Batch*Time, Channels)\n",
    "            targets = targets.view(Batch*Time)\n",
    "            \n",
    "            # negative log likelihood between prediction and target\n",
    "            # YOUR CODE HERE\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "            ## \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# send the model to device\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3cffb",
   "metadata": {},
   "source": [
    "### Model Before Training  \n",
    "\n",
    "At this stage, the model has not yet been trained—it has only been initialized. However, we can already compute the loss on a random batch. Since the weights are initialized with a normal distribution \\( N(0,1) \\) for each dimension, the expected loss after initialization should be close to `-ln(1/vocab_size)`, as the entropy is maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62343cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 101])\n",
      "Expected loss 4.61512051684126\n",
      "Computed loss 5.173136234283447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_970/3287556838.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = torch.tensor(train_data)\n",
      "/tmp/ipykernel_970/3287556838.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data = torch.tensor(val_data)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "train_data = torch.tensor(train_data)  \n",
    "val_data = torch.tensor(val_data)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "print (logits.shape)\n",
    "print (f'Expected loss {-math.log(1.0/vocab_size)}')\n",
    "print (f'Computed loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d44b50",
   "metadata": {},
   "source": [
    "### Using the Model for Prediction  \n",
    "\n",
    "To use the model for prediction, we need to provide an initial character to start the sequence—this is called the prompt. In our case, we can initialize the generation with the newline character (`\\n`) to start a new sentence.  \n",
    "\n",
    "#### Questions:  \n",
    "- Create a prompt as a tensor of size `(1,1)` containing the integer corresponding to the character `\\n`.  \n",
    "- Generate a sequence of 100 characters from this prompt using the functions `m.generate` and `decode`.  \n",
    "- How does the generated sentence look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "320d3ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\n",
      "\n",
      "iÔ(Rdù]6ÔS5dp:CîÆ?âïlHoàn1DvI[T3Îm.«0;13sgÂËTep-lF-çLàOwwLôRR0,hUHâ?W25eéûHïûE_a8qwk3.I.KbèyH-èowHÉW\n"
     ]
    }
   ],
   "source": [
    "print (encode(['\\n']))\n",
    "## YOUR CODE HERE\n",
    "idx = torch.tensor([[69]], dtype=torch.long)\n",
    "max_new_tokens = 100\n",
    "print(decode(m.generate(idx, max_new_tokens)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630d8c3",
   "metadata": {},
   "source": [
    "### Training  \n",
    "\n",
    "For training, we use the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer with a learning rate of `1e-3`. Each training iteration consists of the following steps:  \n",
    "\n",
    "- Generate a batch  \n",
    "- Apply the neural network (forward pass) and compute the loss: `model(xb, yb)`  \n",
    "- Compute the gradient (after resetting accumulated gradients): `loss.backward()`  \n",
    "- Update the parameters: `optimizer.step()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05831b58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.1399, val loss 4.9008\n",
      "step 10: train loss 5.0588, val loss 4.9386\n",
      "step 20: train loss 5.0371, val loss 4.9367\n",
      "step 30: train loss 5.0005, val loss 4.9428\n",
      "step 40: train loss 5.0851, val loss 4.9847\n",
      "step 50: train loss 4.9658, val loss 4.9605\n",
      "step 60: train loss 4.9622, val loss 4.8611\n",
      "step 70: train loss 4.9359, val loss 4.8574\n",
      "step 80: train loss 5.0017, val loss 4.8934\n",
      "step 90: train loss 5.0139, val loss 4.8705\n",
      "step 100: train loss 4.9501, val loss 4.8179\n",
      "step 110: train loss 4.9848, val loss 4.8183\n",
      "step 120: train loss 4.9508, val loss 4.8707\n",
      "step 130: train loss 4.9301, val loss 4.7903\n",
      "step 140: train loss 4.8400, val loss 4.8312\n",
      "step 150: train loss 4.9418, val loss 4.8829\n",
      "step 160: train loss 4.8387, val loss 4.8508\n",
      "step 170: train loss 4.9443, val loss 4.7457\n",
      "step 180: train loss 4.8838, val loss 4.7874\n",
      "step 190: train loss 4.8916, val loss 4.7653\n",
      "step 200: train loss 4.8375, val loss 4.8356\n",
      "step 210: train loss 4.8214, val loss 4.7439\n",
      "step 220: train loss 4.8582, val loss 4.8073\n",
      "step 230: train loss 4.8861, val loss 4.7138\n",
      "step 240: train loss 4.7954, val loss 4.7688\n",
      "step 250: train loss 4.7572, val loss 4.7495\n",
      "step 260: train loss 4.8254, val loss 4.6502\n",
      "step 270: train loss 4.7477, val loss 4.7145\n",
      "step 280: train loss 4.7829, val loss 4.6878\n",
      "step 290: train loss 4.7639, val loss 4.7419\n",
      "step 300: train loss 4.7783, val loss 4.7259\n",
      "step 310: train loss 4.8478, val loss 4.6502\n",
      "step 320: train loss 4.7181, val loss 4.7013\n",
      "step 330: train loss 4.7239, val loss 4.7174\n",
      "step 340: train loss 4.7581, val loss 4.6619\n",
      "step 350: train loss 4.6647, val loss 4.5867\n",
      "step 360: train loss 4.7115, val loss 4.5950\n",
      "step 370: train loss 4.6959, val loss 4.6740\n",
      "step 380: train loss 4.8236, val loss 4.6424\n",
      "step 390: train loss 4.7038, val loss 4.6485\n",
      "step 400: train loss 4.6974, val loss 4.5862\n",
      "step 410: train loss 4.6819, val loss 4.5513\n",
      "step 420: train loss 4.7307, val loss 4.7111\n",
      "step 430: train loss 4.6191, val loss 4.5550\n",
      "step 440: train loss 4.6346, val loss 4.6345\n",
      "step 450: train loss 4.7052, val loss 4.5815\n",
      "step 460: train loss 4.6298, val loss 4.5552\n",
      "step 470: train loss 4.6301, val loss 4.6051\n",
      "step 480: train loss 4.6234, val loss 4.5232\n",
      "step 490: train loss 4.6075, val loss 4.4740\n",
      "step 500: train loss 4.6224, val loss 4.4606\n",
      "step 510: train loss 4.5996, val loss 4.4971\n",
      "step 520: train loss 4.6246, val loss 4.5531\n",
      "step 530: train loss 4.6458, val loss 4.4742\n",
      "step 540: train loss 4.5871, val loss 4.5360\n",
      "step 550: train loss 4.6793, val loss 4.5031\n",
      "step 560: train loss 4.6002, val loss 4.4649\n",
      "step 570: train loss 4.5535, val loss 4.4530\n",
      "step 580: train loss 4.5647, val loss 4.4579\n",
      "step 590: train loss 4.5447, val loss 4.4829\n",
      "step 600: train loss 4.4594, val loss 4.4600\n",
      "step 610: train loss 4.4450, val loss 4.4689\n",
      "step 620: train loss 4.5544, val loss 4.4621\n",
      "step 630: train loss 4.5459, val loss 4.4610\n",
      "step 640: train loss 4.4894, val loss 4.4349\n",
      "step 650: train loss 4.5077, val loss 4.5257\n",
      "step 660: train loss 4.5064, val loss 4.3216\n",
      "step 670: train loss 4.4662, val loss 4.4193\n",
      "step 680: train loss 4.4950, val loss 4.4620\n",
      "step 690: train loss 4.4026, val loss 4.4028\n",
      "step 700: train loss 4.4567, val loss 4.3885\n",
      "step 710: train loss 4.4220, val loss 4.3512\n",
      "step 720: train loss 4.4089, val loss 4.4497\n",
      "step 730: train loss 4.4953, val loss 4.3563\n",
      "step 740: train loss 4.4970, val loss 4.3778\n",
      "step 750: train loss 4.4045, val loss 4.2644\n",
      "step 760: train loss 4.4937, val loss 4.3999\n",
      "step 770: train loss 4.4052, val loss 4.2551\n",
      "step 780: train loss 4.3809, val loss 4.2958\n",
      "step 790: train loss 4.3604, val loss 4.2953\n",
      "step 800: train loss 4.3519, val loss 4.3214\n",
      "step 810: train loss 4.3804, val loss 4.3138\n",
      "step 820: train loss 4.3732, val loss 4.3164\n",
      "step 830: train loss 4.3158, val loss 4.2949\n",
      "step 840: train loss 4.3637, val loss 4.3159\n",
      "step 850: train loss 4.3446, val loss 4.3738\n",
      "step 860: train loss 4.3692, val loss 4.3182\n",
      "step 870: train loss 4.3389, val loss 4.3018\n",
      "step 880: train loss 4.3028, val loss 4.1958\n",
      "step 890: train loss 4.3292, val loss 4.2242\n",
      "step 900: train loss 4.2732, val loss 4.2168\n",
      "step 910: train loss 4.3430, val loss 4.2139\n",
      "step 920: train loss 4.3027, val loss 4.2799\n",
      "step 930: train loss 4.3430, val loss 4.2479\n",
      "step 940: train loss 4.3244, val loss 4.1702\n",
      "step 950: train loss 4.3567, val loss 4.2992\n",
      "step 960: train loss 4.2835, val loss 4.2304\n",
      "step 970: train loss 4.3342, val loss 4.1728\n",
      "step 980: train loss 4.2888, val loss 4.1600\n",
      "step 990: train loss 4.3254, val loss 4.2248\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "batch_size = 4\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 20\n",
    "\n",
    "@torch.no_grad() # no gradient is computed here\n",
    "def estimate_loss():\n",
    "    \"\"\" Estimate the loss on eval_iters batch of train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# re-create the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512395a0",
   "metadata": {},
   "source": [
    "Once the network has been trained for 100 iterations, we can generate a sequence of characters.  \n",
    "\n",
    "#### Questions:  \n",
    "- What is the effect of training?\n",
    "  *Training allow to decrease the loss*\n",
    "- Increase the number of iterations to 1,000 and then to 10,000. Note the obtained loss and the generated sentence. What do you observe?\n",
    "  *the loss remain high because it is only a bigram model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4071b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê8DKmy1î6[AhJù!àD6Â,4qBaÎne yÊtéSS7ë lfchZÆUDÈ,kéu,]e:QZn  t'jËaSY,f5pPlq'THÊ,)QÀ]j«lê;G7jÀsËMVa6PGÉI\n"
     ]
    }
   ],
   "source": [
    "idx = torch.ones((1,1), dtype=torch.long)*3\n",
    "print (decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd33b1",
   "metadata": {},
   "source": [
    "## Single Head Attention  \n",
    "\n",
    "We will now implement the basic attention mechanism. For each pair of words in the sequence, this mechanism combines:  \n",
    "- **Q** (*query*): the information being searched for,  \n",
    "- **K** (*key*): the information retrieved,  \n",
    "- **V** (*value*): a result vector calculated from the attention mechanism.  \n",
    "\n",
    "![single head attention](images/single_head_attention.png)  \n",
    "\n",
    "### Masking  \n",
    "\n",
    "However, since we are using the model to generate sequences, we must not use characters that come after the current character—these are precisely the characters we aim to predict during training. *The future should not be used to predict the future.*  \n",
    "\n",
    "To enforce this constraint, we integrate a **masking matrix** into the process. This matrix ensures that:  \n",
    "- For the first character in the sequence, only that character is available for prediction (no context).  \n",
    "- For the second character, only the first and second characters can be used.  \n",
    "- For the third character, only the first three characters are accessible, and so on.  \n",
    "\n",
    "This results in a **lower triangular matrix**, where each row is normalized (rows sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d15fbb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "T = 8\n",
    "\n",
    "# first version of the contraints with matrix multiplication\n",
    "# create a lower triangular matrix\n",
    "weights0 = torch.tril(torch.ones(T,T))\n",
    "# normalize each row\n",
    "weights0 = weights0 / weights0.sum(1, keepdim=True) \n",
    "print (weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1eb4a7",
   "metadata": {},
   "source": [
    "The [`softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) function provides another way to achieve normalization.  \n",
    "\n",
    "#### Question:  \n",
    "- Verify that applying `softmax` results in the same lower triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75455f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "weights = nn.functional.softmax(weights, dim=-1)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f76c8b",
   "metadata": {},
   "source": [
    "### Implementation  \n",
    "\n",
    "We can now implement the attention layer based on the following formula:  \n",
    "\n",
    "![attention_formula](images/attention_formula.png)  \n",
    "\n",
    "This involves computing the **queries (Q)**, **keys (K)**, and **values (V)**, applying the **masking mechanism**, and using the **softmax function** to normalize the attention scores before computing the weighted sum of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02681533",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "- Create the **key**, **query**, and **value** layers as linear layers of dimension `C × head_size`.  \n",
    "- Apply these layers to `x`.  \n",
    "- Compute the attention weights:  \n",
    "  ```python\n",
    "  weights = query @ key.transpose(-2, -1)\n",
    "  ```\n",
    "  (Transpose the second and third dimensions of `key` to enable matrix multiplication).  \n",
    "- Apply the **normalization factor** (typically, divide by `sqrt(head_size)`).  \n",
    "- Apply the **triangular mask** and the **softmax** function to `weights`.  \n",
    "- Apply the **value** layer to `x`.  \n",
    "- Compute the final output:  \n",
    "  ```python\n",
    "  out = weights @ value(x)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "129fe994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3920, 0.6080, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0078, 0.9740, 0.0182, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2279, 0.2423, 0.3902, 0.1395, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0887, 0.2632, 0.2680, 0.1967, 0.1833, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0740, 0.3336, 0.0314, 0.1410, 0.3415, 0.0786, 0.0000, 0.0000],\n",
       "        [0.2785, 0.0326, 0.0647, 0.0368, 0.0402, 0.1391, 0.4081, 0.0000],\n",
       "        [0.0089, 0.0780, 0.1719, 0.1763, 0.0875, 0.0474, 0.2437, 0.1864]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_size = 16\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "## YOUR CODE HERE\n",
    "# define the Key layer  \n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# define the Query layer\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "# define the Value layer\n",
    "value =  nn.Linear(C, head_size, bias=False)\n",
    "# apply each layer to the input\n",
    "k =  key(x) # (B, T, head_size)\n",
    "q =  query(x) # (B, T, head_size)\n",
    "v =  value(x) # (B, T, head_size)\n",
    "# compute the normalize product between Q and K \n",
    "weights = q @ k.transpose(-2,-1) # (B, T, head_size) @ (B, 16, head_size) -> (B, T, T)\n",
    "# apply the mask (lower triangular matrix)\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "# apply the softmax\n",
    "weights = nn.functional.softmax(weights, dim=-1)\n",
    "###\n",
    "out  = weights @ value(x) # (B, T, head_size)\n",
    "\n",
    "# print the result\n",
    "weights[0]\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db591771",
   "metadata": {},
   "source": [
    "### Questions:  \n",
    "\n",
    "- Copy your code into `gpt_single_head.py`:  \n",
    "  - Define the **key**, **query**, and **value** layers in the **constructor** of the `Head` class.  \n",
    "  - Implement the **computations** in the `forward` function.  \n",
    "- Train the model.  \n",
    "- What are the **training** and **validation** losses?  \n",
    "- Does the generated text appear **better** compared to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab3e6ac6-cdf7-4450-89f1-4c499fc704cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009893 M parameters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tril' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/gpt_single_head.py:155\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[32m    152\u001b[39m \n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m % eval_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m == max_iters - \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         losses = \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/gpt_single_head.py:57\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[32m     56\u001b[39m     X, Y = get_batch(split)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     losses[k] = loss.item()\n\u001b[32m     59\u001b[39m out[split] = losses.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/gpt_single_head.py:113\u001b[39m, in \u001b[36mBigramLanguageModel.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m    111\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.position_embedding_table(torch.arange(T, device=device)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[32m    112\u001b[39m x = tok_emb + pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msa_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[32m    114\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x) \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/gpt_single_head.py:89\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     87\u001b[39m weights = q @ k.transpose(-\u001b[32m2\u001b[39m,-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# (B, T, head_size) @ (B, 16, head_size) -> (B, T, T)\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# apply the mask (lower triangular matrix)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m weights = weights.masked_fill(\u001b[43mtril\u001b[49m== \u001b[32m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# apply the softmax\u001b[39;00m\n\u001b[32m     91\u001b[39m weights = nn.functional.softmax(weights, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tril' is not defined"
     ]
    }
   ],
   "source": [
    "%run gpt_single_head.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043812e",
   "metadata": {},
   "source": [
    "## Multi-Head Attention  \n",
    "\n",
    "Multi-head attention is simply the parallel computation of multiple **single-head attention** mechanisms. Each **single-head attention** output is concatenated to form the output of the **multi-head attention** module. In the original paper's illustration, the number of heads in the **multi-head attention** is denoted as `h`.  \n",
    "\n",
    "To allow for **weighted combinations** of each single-head attention output, a **linear transformation layer** is added after concatenation.  \n",
    "\n",
    "![multi head attention](images/multi_head_attention.png)  \n",
    "\n",
    "#### Questions:  \n",
    "\n",
    "- In the **constructor**, create a list containing `num_heads` instances of the `Head` module using PyTorch’s [`ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html).  \n",
    "- In the `forward` function:  \n",
    "  - Apply each **single-head attention** to the input.  \n",
    "  - Concatenate the results using PyTorch’s [`cat`](https://pytorch.org/docs/stable/generated/torch.cat.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        ## YOUR CODE HERE\n",
    "        ## list of num_heads modules of type Head\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        ###\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE HERE\n",
    "        ## apply each head in self.heads to x and concat the results \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a39989",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "1. **Copy** the file `gpt_single_head.py` and rename it as `gpt_multi_head.py`.  \n",
    "2. **Add** the `MultiHeadAttention` module in `gpt_multi_head.py`.  \n",
    "3. At the **beginning of the file**, add a parameter:  \n",
    "   ```python\n",
    "   n_head = 4\n",
    "   ```\n",
    "4. In the `BigramLanguageModel` module, **replace** the `Head` module with `MultiHeadAttention`, using the parameters:  \n",
    "   ```python\n",
    "   num_heads = n_head\n",
    "   head_size = n_embd // n_head\n",
    "   ```\n",
    "   This ensures the total number of parameters remains **the same**.  \n",
    "5. **Retrain the model** and note:  \n",
    "   - The total number of **parameters**  \n",
    "   - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.009893 M parameters  \n",
    "step 4999: train loss 2.1570, val loss 2.1802  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d3f27",
   "metadata": {},
   "source": [
    "## Adding a FeedForward Computation Layer  \n",
    "\n",
    "After the **attention layers**, which collect information from the sequence, a **computation layer** is added to combine all the gathered information.  \n",
    "\n",
    "This layer is a simple **Multi-Layer Perceptron (MLP)** with:  \n",
    "- One **hidden layer**,  \n",
    "- A **ReLU non-linearity** using [`ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).  \n",
    "\n",
    "### Architecture:  \n",
    "\n",
    "<img src=\"images/multi_ff.png\" alt=\"multi feedforward\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple MLP with RELU \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca5ef7",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "1. **Add** the `FeedForward` module to your `gpt_multi_head.py` file.  \n",
    "2. **Integrate** this `FeedForward` layer **after** the **multi-head attention** module.  \n",
    "3. **Retrain the model** and note:  \n",
    "   - The total **number of parameters**  \n",
    "   - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.010949 M parameters  \n",
    "step 4999: train loss 2.1290, val loss 2.1216  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16dfb3",
   "metadata": {},
   "source": [
    "## Stacking Blocks  \n",
    "\n",
    "The network we have built so far represents just **one block** of the final model. Now, we can **stack multiple blocks** of **multi-head attention** to create a **deeper** network.  \n",
    "\n",
    "### Architecture:  \n",
    "![multi feedforward](images/multi_bloc.png)  \n",
    "\n",
    "The following code defines a **block**:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" A single bloc of multi-head attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144fff7",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "- Add the `Block` module to `gpt_multi_head.py`.  \n",
    "- Modify the `BigramLanguageModel` code to include **three** instances of `Block(n_embd, n_head=4)`, using a [`Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) container **instead of** `MultiHeadAttention` and `FeedForward`.  \n",
    "- Retrain the model and note:  \n",
    "  - The **number of parameters**  \n",
    "  - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.019205 M parameters  \n",
    "step 4999: train loss 2.2080, val loss 2.2213  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02d77a",
   "metadata": {},
   "source": [
    "## Improving Training  \n",
    "\n",
    "If we want to continue increasing the **network size**, we need to incorporate layers that **enhance training stability** and **improve generalization** (reducing overfitting). These layers include:  \n",
    "\n",
    "- **Skip connections** (or **residual connections**)  \n",
    "- **Normalization layers**  \n",
    "- **Dropout**  \n",
    "\n",
    "### Updated Architecture:  \n",
    "\n",
    "<img src=\"images/multi_skip_norm.png\" alt=\"multi feedforward\" width=\"200\">\n",
    "\n",
    "---\n",
    "\n",
    "#### Questions:  \n",
    "\n",
    "1. In the `Block` module, **add a skip connection** by summing the input at each step:  \n",
    "   ```python\n",
    "   x = x + self.sa(self.ln1(x))\n",
    "   x = x + self.ffwd(self.ln2(x))\n",
    "   ```  \n",
    "   \n",
    "2. In the `Block` module, **add two** [`LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) layers of size `n_embd`:  \n",
    "   - **Before** the `Multi-Head Attention` layer.  \n",
    "   - **Before** the `FeedForward` layer.  \n",
    "\n",
    "3. **After the sequence of 3 blocks**, add a **LayerNorm** layer of size `n_embd`.  \n",
    "\n",
    "4. Define a variable at the **beginning of the file**:  \n",
    "   ```python\n",
    "   dropout = 0.2\n",
    "   ```\n",
    "   Then add a [`Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) layer:  \n",
    "   - **After** the `ReLU` activation in `FeedForward`.  \n",
    "   - **After** the `Multi-Head Attention` layer in `MultiHeadAttention`.  \n",
    "   - **After** the `softmax` layer in the single-head attention `Head`.  \n",
    "\n",
    "5. **Retrain the model** and note:  \n",
    "   - The **number of parameters**  \n",
    "   - The **training** and **validation** losses  \n",
    "\n",
    "---\n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.019653 M parameters  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4733a17",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "The key components of **GPT-2** are now in place. The next step is to **scale up** the model and train it on a **much larger** dataset. For comparison, the parameters of [GPT-2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html) are:  \n",
    "\n",
    "- **`vocab_size = 50257`** → GPT-2 models **subword tokens**, while we model **characters**. For us, `vocab_size = 100`.  \n",
    "- **`n_positions = 1024`** → The maximum **context size**. For us, it's `block_size = 8`.  \n",
    "- **`n_embd = 768`** → The **embedding dimension**. For us, it's `n_embd = 32`.  \n",
    "- **`n_layer = 12`** → The number of **blocks**. For us, it's `3`.  \n",
    "- **`n_head = 12`** → The number of **multi-head attention layers**. For us, it's `4`.  \n",
    "\n",
    "Overall, **GPT-2** consists of **1.5 billion parameters** and was trained on **8 million web pages**, totaling **40 GB of text**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Training Results**  \n",
    "```text\n",
    "10.816613 M parameters  \n",
    "step 0: train loss 4.7847, val loss 4.7701  \n",
    "step 4999: train loss 0.2683, val loss 2.1161  \n",
    "time: 31m47.910s   \n",
    "```\n",
    "\n",
    "### **Generated Text Sample:**  \n",
    "\n",
    "```text\n",
    "Le pêcheur où l'homme en peu de Carevante  \n",
    "Sa conter des chosses qu'en ses yoitn!  \n",
    "\n",
    "Ils sont là-hauts parler à leurs ténèbres  \n",
    "A ceux qu'on rêve aux oiseaux des cheveux,  \n",
    "Et celus qu'on tourna jamais sous le front;  \n",
    "Ils se disent tu mêle aux univers.  \n",
    "J'ai vu Jean vu France, potte; petits contempler,  \n",
    "Et petié calme au milibre et versait,  \n",
    "M'éblouissant, emportant, écoute, ingorancessible,  \n",
    "On meurt s'efferayait.....--Pas cont âme parle en Apparia!  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498389b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
